{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# htrc-vectorize\n",
    "\n",
    "This notebook demonstrates how to use HTRC extracted features to build a word2vec model using Doc2Vec. \n",
    "\n",
    "Jed Dobson<br>\n",
    "James.E.Dobson@Dartmouth.EDU<br>\n",
    "http://www.dartmouth.edu/~jed<br>\n",
    "October 2020<br>\n",
    " \n",
    "This file is part of the htrc-vector-project, begun in June 2020 with Catherine Parnell. \n",
    "\n",
    "<b>Repository</b>:<br>\n",
    "https://github.com/jeddobson/htrc-vector-project\n",
    "\n",
    "### Why use HTRC Features?\n",
    "\n",
    "The <a href=\"https://analytics.hathitrust.org/features\">HTRC Extracted Features dataset</a> include over 17 million volumes. This includes works presently still protected by copyright. These features can be distributed because they are for non-consumptive use. They are designed to be computer rather than human readable. This means that you can model large number of texts including texts from the twenty and twenty-first century. Document and page-based features are available. The <a href=\"https://github.com/htrc/htrc-feature-reader\">HTRC feature-reader package</a> for Python enables easy access to these features. These features include tokens and their repititions on a page-by-page basis. Word order is lost (thus the page is no longer human readable). The format used limits their use, however, and many popular applications of text mining and machine learning expect preserved word order. The popular Skipgram model used by <a href=\"https://github.com/tmikolov/word2vec\">word2vec</a>, for example, learns by predicting words within a window surrounding a target word. \n",
    "    \n",
    "### Doc2Vec\n",
    "This notebook uses <a href=\"https://radimrehurek.com/gensim/models/doc2vec.html\">Doc2Vec</a> to train a model for words appearing within a much larger window. Doc2Vec has been used to produce vectors from paragraph-length text sources. These vectors are then used for classification and other applications.\n",
    "\n",
    "### Tunable and Limitations\n",
    "This example notebook uses a set of Toni Morrison novels found in the HATHI Trust archive. This is a much smaller dataset than we've used in other experiments. Vector models like word2vec generally require larger numbers of text sources (we've trained on 30GB archives). \n",
    "\n",
    "<b>Text Sources</b>\n",
    "- input size: Depending on what you want to model, you will most likely want to work with more than words written by a single author. We successfully used this approach with thousands of texts across multiple genres.\n",
    "- preprocessing: This notebook does not provide any preprocessing and simply imports all extracted features. You may want to remove some words with high frequent use (i.e., \"stopwords\"). Results may vary.\n",
    "\n",
    "<b>Execution</b>\n",
    "In real execution, you'll want to separate the several phases of this workflow. You can predownload the features and load these locally (this script will download as needed). To manage run time, you may also want to process each text individually. We produced a CSV file with one line for each page of the text. These were then concatenated (and compressed with bzip) and converted into a TaggedDocument before running doc2vec.\n",
    "\n",
    "<b>doc2vec</b>\n",
    "- window: This is the primary tunable. The smaller this variable the more likely you are to find similar word vectors based on alphabetical ordering (the tokens are processed in alphabetical ordering, as received from the feature-reader). The window size should approximate the typical page length.\n",
    "- min_count: Most errors introduced during digitization or creation of the extracted features should be removed with a small integer used here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.keyedvectors as kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "documents = [\n",
    "    [\"The Bluest Eye\",\"uc1.32106018657251\"],\n",
    "    [\"Song of Solomon\",\"mdp.39015032749130\"],\n",
    "    [\"Sula\",\"uc1.32106019072633\"],\n",
    "    [\"Tar Baby\",\"uc1.32106005767956\"],\n",
    "    [\"Jazz\",\"ien.35556029664190\"],\n",
    "    [\"Beloved\",\"mdp.49015003142743\"],\n",
    "    [\"Paradise\",\"mdp.39015066087613\"],\n",
    "    [\"A Mercy\",\"mdp.39076002787351\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for d in documents:\n",
    "#    path = utils.id_to_rsync(d[1])\n",
    "#    path = \"data.analytics.hathitrust.org::features-2020.03/\" + path\n",
    "#    !rsync -av {path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts individual pages and create string of words from tokens\n",
    "# Word order is lost from HTRC features. This creates page length strings by\n",
    "# multiplying tokens for each appearance. Thus, token the with count 2 will \n",
    "# appear as \"the the\" in the returned string.\n",
    "\n",
    "def get_pages(document):\n",
    "    fr = FeatureReader([document])\n",
    "    vol = next(fr.volumes())\n",
    "    ptc = vol.tokenlist(pos=False, case=False).reset_index().drop(['section'], axis=1)\n",
    "    page_list = set(ptc['page'])\n",
    "    \n",
    "    rows=list()\n",
    "    for page in page_list:\n",
    "        page_data = str()\n",
    "        \n",
    "        # operate on each token\n",
    "        for page_tokens in ptc.loc[ptc['page'] == page].iterrows():\n",
    "            if page_tokens[1][1].isalpha():\n",
    "                page_data += (' '.join([page_tokens[1][1]] * page_tokens[1][2])) + \" \"\n",
    "\n",
    "        # Doc2Vec needs comma separated list of words\n",
    "        rows.append(page_data.split())\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process downloaded features and store as TaggedDocument with a tag for page number\n",
    "# This tage is required for Doc2Vec and would normally be based on paragraphs but we\n",
    "# can only operate on pages of data from HTRC extracted features\n",
    "#\n",
    "\n",
    "pages = list()\n",
    "for d in documents:\n",
    "    for page in get_pages(d[1]):\n",
    "        pages.append(page)\n",
    "\n",
    "# convert to TaggedDocument\n",
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "saving word2vec model\n"
     ]
    }
   ],
   "source": [
    "print(\"creating model\")\n",
    "model = Doc2Vec(tagged_data, \n",
    "                dm=1, # operate on \"paragraphs\" (pages) with distributed memory model\n",
    "                vector_size=300, # larger vector size might produce better results\n",
    "                min_count=5, # drop words with very few repetitions\n",
    "                window=150, # larger window size needed because of extracted features\n",
    "                workers=2)\n",
    "\n",
    "print(\"saving word2vec model\")\n",
    "model.save_word2vec_format(\"doc2vec-morrison-novels.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and verify\n",
    "model =  kv.KeyedVectors.load_word2vec_format(\"doc2vec-morrison-novels.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('possibility', 0.9636703133583069),\n",
       " ('simply', 0.938510000705719),\n",
       " ('quite', 0.9355024099349976),\n",
       " ('possible', 0.9313812255859375),\n",
       " ('ruby', 0.9290497899055481),\n",
       " ('parents', 0.9280668497085571),\n",
       " ('losing', 0.9276642799377441),\n",
       " ('reduced', 0.9182644486427307),\n",
       " ('sold', 0.9174015522003174),\n",
       " ('slaughter', 0.9172781705856323),\n",
       " ('rare', 0.9135681986808777),\n",
       " ('rear', 0.913515031337738),\n",
       " ('shriveled', 0.9132422804832458),\n",
       " ('rain', 0.9129200577735901),\n",
       " ('malice', 0.9115512371063232),\n",
       " ('pond', 0.9083991646766663),\n",
       " ('refused', 0.9077808856964111),\n",
       " ('passed', 0.9076093435287476),\n",
       " ('rejection', 0.9070072770118713),\n",
       " ('transferred', 0.8983979225158691),\n",
       " ('soda', 0.897973895072937),\n",
       " ('presence', 0.8973550200462341),\n",
       " ('sunrise', 0.896835207939148),\n",
       " ('sign', 0.8936558961868286),\n",
       " ('met', 0.8929139375686646)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar([\"memory\"],topn=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
