{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Cultural Analytics: Homework #1</h1></center>\n",
    "<center><h2>ENGL 64.05 / QSS 30.16, 22F</h2></center>\n",
    "<br>\n",
    "<center><b>Due</b> 11:59PM 09/26/2022</center>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lines tell the Python interpreter to load some additional libraries\n",
    "# and functions. We need these functions to intelligently parse Comma Separated Value \n",
    "# (CSV) data, calculate means, and load a local library for reading term \n",
    "# frequency data.\n",
    "\n",
    "# NOTE: This notebook must be opened and executed from your home directory.\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('shared/ENGL64.05-22F/lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Title Length Analytics\n",
    "# HW1.1\n",
    "#####################################################\n",
    "\n",
    "#\n",
    "# We are going to examine a list of metadata from the data that was used\n",
    "# to make the arguments in Ted Underwood's first chapter of _Distant Horizons_\n",
    "#\n",
    "#\n",
    "\n",
    "# we'll set a variable to hold the number of records (ln for line number?) \n",
    "# read from this Comma Separated Value (CSV) file.\n",
    "ln = 0\n",
    "\n",
    "# as above, we'll create a metadata variable \n",
    "metadata = list()\n",
    "\n",
    "# Now we open the CSV file and read each line, appending to the metadata list:\n",
    "with open('shared/ENGL64.05-22F/data/Underwood_ch1/allgenremeta.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter = ',')\n",
    "    for row in reader:\n",
    "        metadata.append(row)\n",
    "        \n",
    "        # increment our counter\n",
    "        ln += 1\n",
    "\n",
    "# tell us how many entries we've read\n",
    "print(\"read {0} lines\".format(ln))\n",
    "\n",
    "# Display header (our \"metadata\" for our metadata)\n",
    "print(metadata[0])\n",
    "\n",
    "# Display index for ease in processing\n",
    "print(\"Column numbers:\",[x for x in range(len(metadata[0]))])\n",
    "\n",
    "# And now let's remove the header for easier processing:\n",
    "metadata = metadata[1:]\n",
    "\n",
    "# Display random entry\n",
    "print(\"Random entry:\",random.sample(metadata,1))\n",
    "\n",
    "print(\"Earliest date:\",min([x[2] for x in metadata]))\n",
    "print(\"Latest date:\",max([x[2] for x in metadata]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.1: Title Length Change over Time\n",
    "\n",
    "In the cell below, write code to determine if title lengths (in terms of <b>word count</b>) change over time within this dataset. There are several different ways you could produce this data.\n",
    "\n",
    "<b>Hints:</b>\n",
    "1. If you wish, you can iterate through a list sorted by publication date by operating on the output of sorted():\n",
    "<pre>sorted(metadata, key=lambda book: book[2])</pre>\n",
    "2. Remember that we can split a string into words with the split() string method and count with the len() function: \n",
    "<pre>len(title.split())</pre>\n",
    "3. As the publication years are simply stored as strings after reading from the CSV file, you can match decades by comparing the first three characters of a string: <pre>[len(book[10]) for book in metadata if book[2].startswith('186')]</pre>\n",
    "4. You can determine the mean (average) of a list of values with the following imported function:\n",
    "<pre>np.mean(list_name)</pre> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #1: Written response</b> (Don't forget to save your notebook before closing.) In no more than two-hundred words describe what you have discovered, if it aligns with Moretti's claims, and what critical questions you might ask about the data and the method you used.\n",
    "\n",
    "Response goes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Entropic Poem\n",
    "# HW1.2\n",
    "#####################################################\n",
    "\n",
    "# 1) create a new text file with a poem, song lyric, or\n",
    "# other short text.\n",
    "# 2) read the contents of the file using open() and read() \n",
    "# as a string and assign to the variable source_text\n",
    "\n",
    "# preprocess using the Natural Language Toolkit (NLTK) Tokenizer\n",
    "# this is a slightly smarter way of extracting words--better than using \"split\"\n",
    "# as it has some ability to strip punctuation and extract other \"tokens\" of interest.\n",
    "# if you want to learn more about the present method used by NLTK, the code is here:\n",
    "# https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py\n",
    "\n",
    "\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(source_text)\n",
    "\n",
    "# drop to lowercase\n",
    "tokens = [word.lower() for word in tokens]\n",
    "\n",
    "# create our vocabulary dictionary\n",
    "vocab = dict()\n",
    "\n",
    "# now produce the entropic poem\n",
    "for word in tokens:\n",
    "    if word not in vocab:\n",
    "        vocab[word] = tokens.count(word)\n",
    "        print(vocab[word],word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.2: Entropic Poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #2:</b> In no more than two hundred words respond the following prompt: Find some interesting text and save to a new file (New-> Text File) or upload to JupyterHub. Read into a variable titled \"source_text\" (see Lab 1). Why did you select the text that you selected? What did you notice about your \"entropic poem?\" Does it work better with certain kinds of texts? \n",
    "\n",
    "Response goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Type/Token Ratios\n",
    "# HW1.3\n",
    "#####################################################\n",
    "\n",
    "from tsvdro import tsvdro\n",
    "\n",
    "document = tsvdro.load(\"document_goes_here\")\n",
    "tokens = sum([x for x in document['data'].values()])\n",
    "types = len(document['data'])\n",
    "\n",
    "print(\"{0:50s} {1:20s} {2:5s} {3:3s}\".format(\"Title\",\"Author\",\"Genre\",\"TTR\"))\n",
    "\n",
    "print(\"{0:50s} {1:20s} {2:5s} {3:1.3f}\".format(document['header']['bibliographic_data']['title'],\n",
    "                                            document['header']['bibliographic_data']['author_name'],\n",
    "                                            document['header']['bibliographic_data']['genre'],\n",
    "                                            (types/tokens*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment \\#1.3: Type/Token Ratios\n",
    "\n",
    "Select ten texts from Ted Underwood's dataset and calcuate the type/token ratio for the entire text. The above code is complete with the exception of the filename. The filenames are stored in the first element of the metadata list. You can easily obtain some filenames with:\n",
    "<pre>\n",
    "for book in random.choices(metadata,k=15):\n",
    "    print(\"{0:25s} {1:4s} {2:5s} {3:10s}\".format(book[0],book[5],book[9],book[10]))\n",
    "</pre>\n",
    "You'll have to include the directory name in which these files are found and append \".dro\" to each file name. For example, \"nyp.33433082246376\" (Charles Dickens's Bleak House) would be:\n",
    "<pre>\n",
    "shared/ENGL64.05-22F/data/Underwood_ch1/nyp.33433082246376.dro\n",
    "</pre>\n",
    "\n",
    "After calculating the ratio, take a quick look at the contents of the following variable:\n",
    "<pre>document['data']</pre>\n",
    "before moving on to the next file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #3:</b> In no more than two hundred words respond the following prompt: What surprises you about these results? What questions do you have about the dataset? What information do you need to answer these questions?\n",
    "\n",
    "Response goes here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
