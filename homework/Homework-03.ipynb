{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Cultural Analytics: Homework #3</h1></center>\n",
    "<center><h2>ENGL 64.05 / QSS 30.16, 22F</h2></center>\n",
    "<br>\n",
    "<center><b>Due</b> 11:59PM 10/24/2022</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from glob import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Section 1: Single Text Topic Model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find, Download, and Upload a plain-text document to vectorize. \n",
    "# This should have at least several thousand tokens (i.e., a long text). Consider using Project Gutenberg or \n",
    "# another large text archive.\n",
    "\n",
    "# make this variable's value the name of the file that you've uploaded\n",
    "input_text = \"\"\n",
    "\n",
    "# make into a collection of documents of 1,000 words\n",
    "raw_text = open(input_text).read()\n",
    "tokens = word_tokenize(raw_text)\n",
    "\n",
    "segment_length = 1000\n",
    "data_to_vectorize = list()\n",
    "\n",
    "nseg = round(len(tokens) / segment_length)\n",
    "for i in range(nseg):\n",
    "    segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "    data_to_vectorize.append(' '.join(segment))\n",
    "\n",
    "# this will display some basic information about our data\n",
    "print(\"created {0} segments of {1} words to vectorize from {2} total words\".format(len(data_to_vectorize),\n",
    "                                                                                   segment_length,\n",
    "                                                                                   len(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup vectorizer and process text\n",
    "vec = CountVectorizer(input='content',\n",
    "                      stop_words='english',\n",
    "                      lowercase=True)\n",
    "\n",
    "dtm = vec.fit_transform(data_to_vectorize)\n",
    "dc, vc = dtm.shape\n",
    "print(\"read {0} documents with {1} vocabulary\".format(dc,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA Model\n",
    "#\n",
    "# IMPORTANT PARAMETERS:\n",
    "# n_components = number of topics to extract (if topics are too similar, extract more)\n",
    "#\n",
    "\n",
    "simple_lda_model = LatentDirichletAllocation(n_components=5,\n",
    "                                             max_iter=5,\n",
    "                                             learning_method='batch',\n",
    "                                             random_state=1)    \n",
    "# get fitted data and transformed matrix\n",
    "simple_lda_data = simple_lda_model.fit(dtm)\n",
    "\n",
    "# extract the features to a simple list\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "# IMPORTANT VARIABLES:\n",
    "# how many words do we want to extract for each topic?\n",
    "n_words = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now produce the list of topics\n",
    "for topic_idx, topic in enumerate(simple_lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #1:</b>\n",
    "In no more than two hundred words, what do you notice in the above list of words? Do you think these function as \"topics?\" Can you give each of them a name? Did you alter any of the variables? If so, what did you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Section 2: LDA on a Collection</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some data from one of our collections (in the \"data\" directory)\n",
    "# create a variable named \"collection\" that is a list of file names\n",
    "# You'll need to use the glob() function to match some set of files. \n",
    "# Keep the number to no more than fifty files.\n",
    "#\n",
    "# You can use either Novel450 or the DocSouth collection.\n",
    "# These are all located in the shared/ENGL64.05-22F/data directory\n",
    "\n",
    "print(\"found\",len(collection),\"texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_length = 1000\n",
    "\n",
    "data_to_vectorize = list()\n",
    "nc = 0\n",
    "for document in collection:\n",
    "    raw_text = open(document).read()\n",
    "    tokens = word_tokenize(raw_text)\n",
    "\n",
    "    nseg = round(len(tokens) / segment_length)\n",
    "    for i in range(nseg):\n",
    "        segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "        data_to_vectorize.append(' '.join(segment))\n",
    "    nc += 1\n",
    "    \n",
    "print(\"created {0} segments of {1} words to vectorize from {2} total texts\".format(len(data_to_vectorize),\n",
    "                                                                                   segment_length,\n",
    "                                                                                   nc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come back here later....\n",
    "# custom stopword list...\n",
    "# this loads the existing stopwords from Scikit-Learn\n",
    "custom_stop_words = list(stop_words.ENGLISH_STOP_WORDS)\n",
    "\n",
    "# now we need to add values to this list.\n",
    "# how can you update an existing list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words can be 'english' or a variable name that is a list\n",
    "vec = CountVectorizer(input='content',\n",
    "                            stop_words='english',\n",
    "                            lowercase=True) # lowercase is a Boolean True/False\n",
    "\n",
    "dtm = vec.fit_transform(data_to_vectorize)\n",
    "dc, vc = dtm.shape\n",
    "print(\"read {0} documents with {1} vocabulary\".format(dc,vc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA Model\n",
    "#\n",
    "# how many topics should we use?\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=20,               \n",
    "                                      max_iter=5,\n",
    "                                      learning_method='batch',\n",
    "                                      random_state=None)    \n",
    "# get fitted data and transformed matrix\n",
    "lda_data = lda_model.fit(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features to a simple list\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "# how many words do we want to extract for each topic?\n",
    "n_words = 100\n",
    "\n",
    "# now produce topics\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Custom Stop Words\n",
    "\n",
    "Now re-run the above by building a custom stoplist to remove some frequenly used words that you see in the above list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll plot the words from the first topic\n",
    "# You can easily modify this by selecting the component corresponding to the topic number of interest.\n",
    "\n",
    "data = lda_model.components_[0]\n",
    "word_index = data.argsort()[:-20 - 1:-1]\n",
    "\n",
    "x = [feature_names[x] for x in word_index]\n",
    "y = [data[x] for x in word_index]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Topic #0: Important Tokens\")\n",
    "\n",
    "# what should these be?\n",
    "# add labels...\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "import pandas as pd\n",
    "\n",
    "lda_transformed_data = lda_model.fit_transform(dtm)\n",
    "\n",
    "# make lists of topics and texts for labels\n",
    "topics = [\"Topic \" + str(i) for i in range(lda_model.n_components)]\n",
    "texts = [\"Text \" + str(i) for i in range(len(data_to_vectorize))]\n",
    "\n",
    "# put data into a special datatype called a Pandas DataFrame\n",
    "topic_chart = pd.DataFrame(np.round(lda_transformed_data, 3), columns=topics, index=texts)\n",
    "\n",
    "# extract the most dominant topic by searching for largest value\n",
    "topic_chart['Dominant Topic'] = np.argmax(topic_chart.values, axis=1)\n",
    "\n",
    "# Display\n",
    "topic_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select just those texts that are dominated by a topic of interest\n",
    "topic_chart[topic_chart['Dominant Topic'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #2:</b>\n",
    "In no more than two hundred words, respond the following questions: What do you notice about topic model on this collection of documents? What was the result of increasing the number of stop words? How many did you add and did that increase the interpretability of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Section 3: Non-Negative Matrix Factorization (NMF)</font>\n",
    "\n",
    "We're now going to use a different method of generating a topic model. You can read more about this method below:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "\n",
    "We'll want to be looking for differences in the result of the model but make sure to note how we are using TF-IDF values instead of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to TF-IDF frequencies \n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "tfidf = text.TfidfTransformer().fit_transform(dtm)\n",
    "\n",
    "# fit to NMF model\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# n_components is the number of topics (same as LDA)\n",
    "nmf_model = NMF(n_components=15,\n",
    "                init='random', \n",
    "                random_state=0)\n",
    "\n",
    "# fit \n",
    "nmf_output = nmf_model.fit_transform(tfidf)\n",
    "nmf_data = nmf_model.fit(tfidf)\n",
    "\n",
    "# reload feature names\n",
    "feature_names = vec.get_feature_names_out()\n",
    "\n",
    "# max number of words per topic to display\n",
    "n_words = 100\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    print(\"Topic #%d:\" % topic_idx)\n",
    "    for i in topic.argsort()[:-n_words - 1:-1]:\n",
    "        print(\"{0} \".format(feature_names[i]),end=\"\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document â€” Topic Matrix\n",
    "import pandas as pd\n",
    "\n",
    "# column names\n",
    "topicnames = [\"Topic \" + str(i) for i in range(nmf_model.n_components)]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Text \" + str(i) for i in range(len(data_to_vectorize))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(nmf_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['Dominant Topic'] = dominant_topic\n",
    "df_document_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Question #3:</b>\n",
    "In no more than two hundred words, respond the following questions: What differences do you see in the NMF model vs the LDA model? How might you determine which method to use in topic modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This function will produce segments of a desired size from HathiTrust Extract Features data\n",
    "#\n",
    "\n",
    "from htrc_features import FeatureReader  \n",
    "\n",
    "def get_segments(document, pos_nouns = False, segment_length=1000):\n",
    "    fr = FeatureReader([document])\n",
    "    vol = next(fr.volumes())\n",
    " \n",
    "    # noun selector\n",
    "    if pos_nouns == False:\n",
    "        ptc = vol.tokenlist(pos=False, case=False).reset_index().drop(['section'], axis=1)\n",
    "    else:\n",
    "        ptc = vol.tokenlist(pos=True, case=False).reset_index().drop(['section'], axis=1)\n",
    "        pos_tags = [\"NN\",\"NNS\",\"NNP\",\"NNPS\"]\n",
    "        ptc = ptc[ptc['pos'].isin(pos_tags)]\n",
    "        ptc = ptc.drop('pos', axis=1)\n",
    "        \n",
    "    page_list = set(ptc['page'])\n",
    "    \n",
    "    # extract tokens by page \n",
    "    tokens=list()\n",
    "    for page in page_list:\n",
    "        page_data = str()\n",
    "        \n",
    "        # operate on each token\n",
    "        for page_tokens in ptc.loc[ptc['page'] == page].iterrows():\n",
    "            if page_tokens[1][1].isalpha():\n",
    "                \n",
    "                # deal with frequency count by creating correct number of tokens\n",
    "                page_data += (' '.join([page_tokens[1][1]] * page_tokens[1][2])) + \" \"\n",
    "\n",
    "        tokens = tokens + page_data.split()\n",
    "    \n",
    "    # now cut into segments\n",
    "    data_to_vectorize = list()\n",
    "\n",
    "    nseg = round(len(tokens) / segment_length)\n",
    "    for i in range(nseg):\n",
    "        segment = tokens[segment_length*i:segment_length*(i+1)]\n",
    "        data_to_vectorize.append(' '.join(segment))    \n",
    "\n",
    "    return data_to_vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"red\">Section 4: Topic Model of _The Dartmouth_ from HathiTrust</font>\n",
    "\n",
    "You'll need to find the HathiTrust IDs for at least ten volumes of The Dartmouth at hathitrust.org. These IDs should contained as a list of strings in the texts_to_model variable.\n",
    "\n",
    "<b>NOTE:</b> You will also need to add cells below to vectorize the tokens (data_to_vectorize) that we are extracting here and then create the topic models. You may use either the LDA or NMF model. You can select only nouns, if you wish by modifying the parameters for the get_segments function.\n",
    "\n",
    "Once you've finished this, in no more than two hundred and fifty words, give an account of how successful you think this experiment in topic modeling has been. Are your models of these texts useful? How might you improve your results? What uses and limitations do you see in topic modeling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a list of HathiTrust IDs \n",
    "\n",
    "texts_to_model = \n",
    "\n",
    "# extract tokens and \n",
    "data_to_vectorize = list()\n",
    "for text in texts_to_model:\n",
    "    data_to_vectorize = data_to_vectorize + get_segments(text, segment_length=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
