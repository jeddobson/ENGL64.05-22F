{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 / QSS 30.16 Fall 2022</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 6\n",
    "## Neural Language Models and word2vec\n",
    "\n",
    " <center><pre>Created: 05/16/2021; Revised: 10/31/2022</pre></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from gensim import matutils\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google News 200 Model (smaller)\n",
    "google_model = KeyedVectors.load_word2vec_format(\"shared/ENGL64.05-22F/models/google-vectors.w2v\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Interview\" the model\n",
    "vocab_size, dim = google_model.vectors.shape\n",
    "print(\"vocab:\", vocab_size)\n",
    "print(\"depth:\", dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar Terms\n",
    "google_model.most_similar(\"dartmouth\",topn=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distance (cosine similarity, so angle similarity) between dartmouth and hanover?\n",
    "google_model.distance('dartmouth','hanover')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar Terms: We can group terms together in a list to search for neighbors \n",
    "# of this (\"concept?\") group.\n",
    "google_model.most_similar([\"dartmouth\",\"harvard\",\"yale\"],topn=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Try Your Own Similarity Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogical Reasoning Task(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was introduced with the model\n",
    "google_model.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's another version of this query\n",
    "google_model.most_similar(positive=['oslo', 'portugal'], negative=['lisbon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Try Your Own Analogical Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model came with a list of these for several categories. \n",
    "queries = open('shared/ENGL64.05-22F/models/questions-words.txt').readlines()\n",
    "\n",
    "# Let's display the categories:\n",
    "[e.strip() for e in queries if e.startswith(':')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of just capitals and countries\n",
    "capital_queries = [q.strip().split() for q in queries[1:queries.index(': capital-world\\n')]]\n",
    "print(capital_queries[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you determine the accuracy of this model using these supplied queries?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Neighbors in Vector Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_terms_mds(term):\n",
    "    neighbor_vectors=list()\n",
    "    neighbor_words=list()\n",
    "\n",
    "    for word, j in google_model.most_similar(term,topn=15):\n",
    "        neighbor_words.append(word)\n",
    "        neighbor_vectors.append(google_model[word[0]])\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "    dist_matrix = 1 - cosine_similarity(neighbor_vectors)\n",
    "    pos = mds.fit_transform(dist_matrix)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.clf()\n",
    "    plt.title(\"MDS Neighboring Terms for: \" + term)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.scatter(xs, ys, marker = '^')\n",
    "    for i, w in enumerate(neighbor_words):\n",
    "         plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "            textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now execute this function with a search term\n",
    "scatter_terms_mds(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_terms_pca(term):\n",
    "    neighbor_vectors=list()\n",
    "    neighbor_words=list()\n",
    "\n",
    "    for word, j in google_model.most_similar(term,topn=15):\n",
    "        neighbor_words.append(word)\n",
    "        neighbor_vectors.append(google_model[word[0]])\n",
    "   \n",
    "    pca = PCA(n_components=2)\n",
    "    plot_data = pca.fit_transform(neighbor_vectors)\n",
    "    xs, ys = plot_data[:, 0], plot_data[:, 1]\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    plt.clf()\n",
    "    plt.title(\"PCA Neighboring Terms for: \" + term)\n",
    "    plt.style.use('ggplot')\n",
    "    plt.scatter(xs, ys, marker = '^')\n",
    "    for i, w in enumerate(neighbor_words):\n",
    "         plt.annotate(w, xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "            textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now execute this function with a search term\n",
    "scatter_terms_pca(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Our Own Model\n",
    "\n",
    "The cells below will train a word2vec model from HTRC texts using Doc2Vec. This isn't perfect,\n",
    "as we are using a bag-of-words representation of the text on individual pages to train the model.\n",
    "Word2vec typically would be trained on text with intact word order but if we want to create models \n",
    "from sources under copyright, this is our best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.keyedvectors as kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this a list of HathiTrust IDS\n",
    "documents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts individual pages and create string of words from tokens\n",
    "# Word order is lost from HTRC features. This creates page length strings by\n",
    "# multiplying tokens for each appearance. Thus, token the with count 2 will \n",
    "# appear as \"the the\" in the returned string.\n",
    "\n",
    "def get_pages(document):\n",
    "    fr = FeatureReader([document])\n",
    "    vol = next(fr.volumes())\n",
    "    ptc = vol.tokenlist(pos=False, case=False).reset_index().drop(['section'], axis=1)\n",
    "    page_list = set(ptc['page'])\n",
    "    \n",
    "    rows=list()\n",
    "    for page in page_list:\n",
    "        page_data = str()\n",
    "        \n",
    "        # operate on each token\n",
    "        for page_tokens in ptc.loc[ptc['page'] == page].iterrows():\n",
    "            if page_tokens[1][1].isalpha():\n",
    "                page_data += (' '.join([page_tokens[1][1]] * page_tokens[1][2])) + \" \"\n",
    "\n",
    "        # Doc2Vec needs comma separated list of words\n",
    "        rows.append(page_data.split())\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process downloaded features and store as TaggedDocument with a tag for page number\n",
    "# This tage is required for Doc2Vec and would normally be based on paragraphs but we\n",
    "# can only operate on pages of data from HTRC extracted features\n",
    "#\n",
    "\n",
    "pages = list()\n",
    "for d in documents:\n",
    "    for page in get_pages(d):\n",
    "        pages.append(page)\n",
    "\n",
    "# convert to TaggedDocument\n",
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating model\")\n",
    "model = Doc2Vec(tagged_data, \n",
    "                dm=1, # operate on \"paragraphs\" (pages) with distributed memory model\n",
    "                vector_size=300, # larger vector size might produce better results\n",
    "                min_count=5, # drop words with very few repetitions\n",
    "                window=150, # larger window size needed because of extracted features\n",
    "                workers=2)\n",
    "\n",
    "print(\"saving word2vec model\")\n",
    "model.save_word2vec_format(\"doc2vec-htrc-sample.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and verify\n",
    "model =  kv.KeyedVectors.load_word2vec_format(\"doc2vec-htrc-sample.w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Interview\" the model\n",
    "vocab_size, dim = model.vectors.shape\n",
    "print(\"vocab:\", vocab_size)\n",
    "print(\"depth:\", dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some sample queries\n",
    "model.most_similar(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
