{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 / QSS 30.16 22F</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 2\n",
    "## NLTK, Part of Speech Tagging, Named-Entity Recognition, Punctuation, and Segmentation.\n",
    "\n",
    " <center><pre>Created: 10/09/2019; Revised 09/26/2022</pre></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><font color=\"Green\">Part One: Part of Speech Tagging</font></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin by loading up some important libraries/packages\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import nltk\n",
    "\n",
    "# import both the sentence and word tokenizers\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# needed for POS and NER \n",
    "from nltk import pos_tag, ne_chunk\n",
    "\n",
    "# allow for displaying of graphics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's learn about NLTK and its Part of Speech (POS) Tagger. \n",
    "# Locate some sample text (a large paragraph or page-length) and paste here...\n",
    "\n",
    "test_sentence = \"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tokens\n",
    "tokens = word_tokenize(test_sentence)\n",
    "\n",
    "# create NLTK Text object\n",
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What can you do with this object? What functions look useful? Try a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we run the tagger on the tokens:\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The complete list of tag types appears at the bottom of this notebook\n",
    "\n",
    "# Now let's return to the second cell and write some other kinds of sentences.\n",
    "# Experiment with words that could be nouns or verbs depending on context.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"Green\">Part Two: NLTK and Named Entity Recognition</font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the Named Entities that NLTK can recognize:\n",
    "\n",
    "|NER|Example|\n",
    "|------------|-----------|\n",
    "|ORGANIZATION|Georgia-Pacific Corp., WHO|\n",
    "|PERSON|Eddy Bonte, President Obama|\n",
    "|LOCATION|Murray River, Mount Everest|\n",
    "|DATE|June, 2008-06-29|\n",
    "|TIME|two fifty a m, 1:30 p.m.|\n",
    "|MONEY|175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT|twenty pct, 18.75 %|\n",
    "|FACILITY|Washington Monument, Stonehenge|\n",
    "|GPE|South East Asia, Midlothian|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 150 English-language novels in Andrew Piper's Novel450 dataset:\n",
    "for document in sorted(glob.glob(\"shared/ENGL64.05-22F/data/Novel450/EN*\")):\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of these and read it into the variable raw_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, let's determine how long it is (word count) using our new old friend, the word_tokenizer\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 300 words (roughly a page)\n",
    "sample_tokens = tokens[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a NLTK Text object and display:\n",
    "# 1) the top fifty collocations\n",
    "# 2) a concordance for a token of interest \n",
    "# 3) a list of words appearing in similiar context to your token of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the 'Named Entity Chunker' ne_chunk to 'chunk' our tagged \n",
    "# tokens and then apply named entity recongition.\n",
    "ner_data = ne_chunk(pos_tag(sample_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_type = \"PERSON\" # define NER category of interest\n",
    "\n",
    "# we'll make a dictonary to store found Named Entities\n",
    "found_objects = dict()\n",
    "\n",
    "for i in ner_data.subtrees():\n",
    "    if i.label() == ner_type: \n",
    "            ner_object = i[0][0]\n",
    "            if ner_object in found_objects:\n",
    "                found_objects[ner_object] += 1\n",
    "            else:\n",
    "                found_objects[ner_object] = 1\n",
    "\n",
    "top_objects = sorted(found_objects, key=found_objects.get, reverse=True)\n",
    "for i in top_objects:\n",
    "    print(i,found_objects[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now go back and select a different range (different number of pages) of your text. \n",
    "# Then try another text.\n",
    "# How well does this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"Green\">Part Three: Document Segmentation</font></h2>\n",
    "\n",
    "As we just saw, it will be sometimes better to operate a small section of text. We can call these units \"segments\" and produce them automatically. With a standarized set of segments we can better understand changes throughout narrative time (the \"syuzhet\" or emplotted narrative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one of the above texts and (re)read it into the variable raw_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(raw_text)\n",
    "print(\"found\",len(tokens),\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically we pre-determine the number of segments we want created.\n",
    "\n",
    "ns = 100 # how many segments do we want to create?\n",
    "segment_length = int(len(tokens) / ns) # how many words go in each segment?\n",
    "segments = list()\n",
    "for j in range(ns):\n",
    "    seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "    segments.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's begin with tagging the first \"bucket\"\n",
    "pos_data = nltk.pos_tag(segments[0])\n",
    "\n",
    "# find all the proper nouns (NNP)\n",
    "found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "print(len(set(found_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display them\n",
    "found_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our percent of proper nouns per bucket?\n",
    "data_to_plot=list()\n",
    "\n",
    "for s in segments:\n",
    "    total_tokens = len(s)\n",
    "    \n",
    "    # extract Part of Speech data \n",
    "    pos_data = nltk.pos_tag(s)\n",
    "    \n",
    "    # select only objects of interest\n",
    "    found_words = [word for word in pos_data if word[1] == 'NNP']\n",
    "\n",
    "    # add to list\n",
    "    data_to_plot.append((round(len(found_words)/total_tokens * 100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display these percentages over narrative time\n",
    "import matplotlib.pyplot as plt\n",
    "x = np.arange(len(data_to_plot))\n",
    "plt.plot(x, data_to_plot)\n",
    "plt.title(\"Distribution of Proper Nouns\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is this? What can this distribution of the percentage of\n",
    "# proper nouns tell us?\n",
    "\n",
    "# Now go back and change to find foreign words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color=\"Green\">Part Four: Punctuation</font></h2>\n",
    "\n",
    "Let's now compare the use of punctuation in two different authors.\n",
    "\n",
    "1. Select one text from two different authors in the Novel450 dataset\n",
    "2. Read and tokenize file. \n",
    "3. Use punct_count function to obtain dictionary of counts for 1,000 word segments\n",
    "4. Compare use of punctuation marks as mean value of instances in 1,000 word segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_list = [\".\",\",\",\";\",\":\",\"?\",\"!\",\"—\",\"-\",\"[\",\"(\",\"&\",\"/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_count(tokens):\n",
    "    # create segments of 1,000 tokens\n",
    "    segment_length = 1000\n",
    "    ns = int(len(tokens) / segment_length) # how many segments are needed?\n",
    "    segments = list()\n",
    "    for j in range(ns):\n",
    "        seg = tokens[segment_length*j:segment_length*(j+1)]\n",
    "        segments.append(seg)\n",
    "    punct_dict = dict()\n",
    "\n",
    "    # process each segment and count appearance of punctuation marks\n",
    "    for seg in segments:\n",
    "        for p in punctuation_list:\n",
    "            if p not in punct_dict:\n",
    "                punct_dict[p] = [seg.count(p)]\n",
    "            else:\n",
    "                punct_dict[p].append(seg.count(p))\n",
    "    return punct_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tag list:\n",
    "----\n",
    "\n",
    "|Tag|Meaning|\n",
    "|---|-------|\n",
    "|CC|coordinating conjunction|\n",
    "|CD|cardinal digit|\n",
    "|DT|determiner|\n",
    "|EX|existential there|\n",
    "|FW|foreign word|\n",
    "|IN|preposition/subordinating conjunction|\n",
    "|JJ|adjective|\n",
    "|JJR|adjective, comparative|\n",
    "|JJS|adjective, superlative|\n",
    "|LS|list marker|\n",
    "|MD|modal|\n",
    "|NN|noun, singular|\n",
    "|NNS|noun plural|\n",
    "|NNP|proper noun, singular|\n",
    "|NNPS|proper noun, plural|\n",
    "|PDT|predeterminer|\n",
    "|POS|possessive ending|\n",
    "|PRP|personal pronoun|\n",
    "|PRP$|possessive pronoun|\n",
    "|RB|adverb|\n",
    "|RBR|adverb, comparative|\n",
    "|RBS|adverb, superlative|\n",
    "|RP|particle|\n",
    "|TO|to go|\n",
    "|UH|interjection|\n",
    "|VB|verb, base form|\n",
    "|VBD|verb, past tense|\n",
    "|VBG|verb, gerund/present participle|\n",
    "|VBN|verb, past participle|\n",
    "|VBP|verb, sing. present|\n",
    "|VBZ|verb, 3rd person sing. present|\n",
    "|WDT|wh-determiner which|\n",
    "|WP|wh-pronoun who, what|\n",
    "|WP\\$|possessive pronoun|\n",
    "|WRB|wh-abverb where, when|\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
