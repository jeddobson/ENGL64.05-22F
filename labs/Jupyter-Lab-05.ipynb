{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Cultural Analytics</h1><br>\n",
    "<h2>ENGL64.05 / QSS 30.16 22F</h2>\n",
    "</center>\n",
    "\n",
    "----\n",
    "\n",
    "# Lab 5: Machine Learning and Sentiment Analysis\n",
    "\n",
    " <center><pre>Created: 05/13/2021; Updated 10/24/2022</pre></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Experiment with TextBlob's built-in Sentiment Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Montfort (pp. 201-204) explains how to use TextBlob's included sentiment analysis tool\n",
    "# In the cell below, use this to evaluate the sentiment and subjectivity of several sample \n",
    "# sections of text. Search for text from news reports, tweets, web site text, \n",
    "# fictional texts, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine how the sentiment was calculated with the \"sentiment_assessments\" method. Try other \n",
    "# texts. What do you notice about these terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1: Construct a Scaled Sentiment System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to convert the sentiment score from TextBlob into a four-star system for \n",
    "# determining the degree to which you've found a positive section of text. Create a new\n",
    "# function using def to call TextBlob and return a positivity score rendered in the \n",
    "# four-star scale.\n",
    "#\n",
    "# HINT: You can print an emoji (!) with cut-and-paste. If you want to display multiple \n",
    "# characters you can use string multiplication: \"hey\" * 10 will print \"hey\" ten times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2: Building Our Own Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's native storage format for saving serialized\n",
    "# data objects is called \"pickle.\" We can \"dump\" and \"load\"\n",
    "# objects from these files. This file contains a set of Goodreads\n",
    "# reviews pulled from the Web.\n",
    "\n",
    "import pickle\n",
    "reviews = pickle.load(open(\"shared/ENGL64.05-22F/data/book-reviews.pkl\",\"rb\"))\n",
    "print(\"loaded {0} book reviews\".format(len(reviews)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing:\n",
    "# The reviews contain \"newline\" characters, so let's drop those:\n",
    "reviews = [r.replace(\"\\n\",\"\") for r in reviews]\n",
    "\n",
    "# Remove quotation marks for clarity and ease of processing\n",
    "reviews = [r.replace(\"'\",\"\") for r in reviews]\n",
    "reviews = [r.replace('\"',\"\") for r in reviews]\n",
    "              \n",
    "# Do you want to make everything lowercase? What else might you do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into training and testing datasets.\n",
    "# The training set will contain the first 500 reviews\n",
    "# and the testing set will be the remainder\n",
    "training_reviews = reviews[:500]\n",
    "testing_reviews = reviews[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here k should be the number of samples that we want to extract\n",
    "# you should select a large enough number to contain both positive\n",
    "# and negative reviews but not too many to make creating your labeled \n",
    "# training set too large.\n",
    "#\n",
    "# We're also displaying the index within the training dataset, which could\n",
    "# potentially be useful if you are not into cut-and-paste in constructing \n",
    "# your labeled data.\n",
    "\n",
    "from random import sample\n",
    "sample(list(enumerate(training_reviews)), k=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create a list of tuples in which the first item\n",
    "# is the text of the review and the second a label of either \"pos\"\n",
    "# for a positive review or \"neg\" for a negative review. Both items\n",
    "# in the tuple are strings and need to be quoted. The tuples need\n",
    "# to be separated with commas both within and without the tuple. \n",
    "training_sentiments = [\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify, Show Features, and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TextBlob's wrapper for the Naive Bayes classifier\n",
    "# and train on your training sentiment data\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "clf = NaiveBayesClassifier(training_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now review the document with help(NaiveBayesClassifier) to learn\n",
    "# how to display the most informative features for your trained\n",
    "# classifier.\n",
    "#\n",
    "# Do you want to go back and change anything from above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run through some selection of your testing data (testing_reviews) and \n",
    "# classify using prob_classify and display the label and optionally the \n",
    "# assigned probability that it is that label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Classification with SVM\n",
    "\n",
    "Now we'll use the more familiar CountVectorizer to create a bag-of-words model that we can use for classification with an algorithm called Support Vector Machines (SVM). \n",
    "\n",
    "The SGDClassifer model gives us access to a linear model that is quite flexible and yields interpretable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to load saved course reviews from the LayupList\n",
    "old_reviews = json.load(open('shared/ENGL64.05-22F/data/LayupList/old_reviews.json'))\n",
    "new_reviews = json.load(open('shared/ENGL64.05-22F/data/LayupList/new_reviews.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the reviews for each department\n",
    "for d in set([r['department'] for r in new_reviews]):\n",
    "    print(d,len([r for r in new_reviews if r['department'] == d]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a sample way to get comments for one department. Select *at least three*\n",
    "# departments or programs from the above list and assemble lists with these\n",
    "# narrative comments.\n",
    "math_data = [r['comments'] for r in new_reviews if r['department'] == 'MATH']\n",
    "math_data = [' '.join(list(r.values())) for r in math_data if len(r) != 0]\n",
    "math_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data and labels\n",
    "\n",
    "Review the above and decide if you need to do any additional preprocessing of the data. Create labels as a list of simple identifiers (I'd suggest department/program subject code) for each document that you will vectorize. The labels must match the data (item x in the labels should match item x in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you'll need to make training labels (a list that will identify each item vectorized)\n",
    "train_labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to vectorize the data. What options/parameters should we use? \n",
    "\n",
    "# 1: create an instance of CountVectorizer with suitable parameters to vectorize \n",
    "#    content data (list of strings)\n",
    "# 2: call this \"vec\"\n",
    "# 3: create the document-term matrix by fit_transforming your data to your\n",
    "#    instance of CountVectorizer.\n",
    "# 4: call this \"dtm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the shape? \n",
    "#\n",
    "# documents x vocabulary\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using SVM\n",
    "clf = SGDClassifier(tol=None,max_iter=1000).fit(dtm, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within Scikit-learn, the weights are stored in the \"coef_\" attribute for the classifier.\n",
    "# This is a numpy array with the dimensions of the classes and total vocabulary (i.e., dtm.shape[1])\n",
    "clf.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most important/influential features as weights can be extracted by sorting.\n",
    "# We can get the top ten more important features for the first class with the following:\n",
    "np.argsort(clf.coef_[0])[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These the vocabulary indices for the terms (also index for obtaining values from the coefficients/weights. \n",
    "# We can lookup the feature (term/token) from the vectorizer, which maintains the mapping \n",
    "# in get_feature_names_out()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the most important features to learning the correct classification of these reviews\n",
    "feat_number = 10\n",
    "feature_names = vec.get_feature_names_out()\n",
    "for i, class_label in enumerate(clf.classes_):\n",
    "    terms = np.argsort(clf.coef_[i])[-feat_number:]\n",
    "    print(\"%s: %s\" % (class_label,\n",
    "                      \", \".join(feature_names[j] for j in terms)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
