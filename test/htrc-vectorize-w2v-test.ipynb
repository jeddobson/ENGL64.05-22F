{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils  \n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim.models.keyedvectors as kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to download\n",
    "documents = [\n",
    "    [\"The Bluest Eye\",\"uc1.32106018657251\"],\n",
    "    [\"Song of Solomon\",\"mdp.39015032749130\"],\n",
    "    [\"Sula\",\"uc1.32106019072633\"],\n",
    "    [\"Tar Baby\",\"uc1.32106005767956\"],\n",
    "    [\"Jazz\",\"ien.35556029664190\"],\n",
    "    [\"Beloved\",\"mdp.49015003142743\"],\n",
    "    [\"Paradise\",\"mdp.39015066087613\"],\n",
    "    [\"A Mercy\",\"mdp.39076002787351\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts individual pages and create string of words from tokens\n",
    "# Word order is lost from HTRC features. This creates page length strings by\n",
    "# multiplying tokens for each appearance. Thus, token the with count 2 will \n",
    "# appear as \"the the\" in the returned string.\n",
    "\n",
    "def get_pages(document):\n",
    "    fr = FeatureReader([document])\n",
    "    vol = next(fr.volumes())\n",
    "    ptc = vol.tokenlist(pos=False, case=False).reset_index().drop(['section'], axis=1)\n",
    "    page_list = set(ptc['page'])\n",
    "    \n",
    "    rows=list()\n",
    "    for page in page_list:\n",
    "        page_data = str()\n",
    "        \n",
    "        # operate on each token\n",
    "        for page_tokens in ptc.loc[ptc['page'] == page].iterrows():\n",
    "            if page_tokens[1][1].isalpha():\n",
    "                page_data += (' '.join([page_tokens[1][1]] * page_tokens[1][2])) + \" \"\n",
    "\n",
    "        # Doc2Vec needs comma separated list of words\n",
    "        rows.append(page_data.split())\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process downloaded features and store as TaggedDocument with a tag for page number\n",
    "# This tage is required for Doc2Vec and would normally be based on paragraphs but we\n",
    "# can only operate on pages of data from HTRC extracted features\n",
    "#\n",
    "\n",
    "pages = list()\n",
    "for d in documents:\n",
    "    for page in get_pages(d[1]):\n",
    "        pages.append(page)\n",
    "\n",
    "# convert to TaggedDocument\n",
    "tagged_data = [TaggedDocument(words=_d, tags=[str(i)]) for i, _d in enumerate(pages)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"creating model\")\n",
    "model = Doc2Vec(tagged_data, \n",
    "                dm=1, # operate on \"paragraphs\" (pages) with distributed memory model\n",
    "                vector_size=300, # larger vector size might produce better results\n",
    "                min_count=5, # drop words with very few repetitions\n",
    "                window=150, # larger window size needed because of extracted features\n",
    "                workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar([\"memory\"],topn=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
